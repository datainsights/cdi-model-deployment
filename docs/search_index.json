[["index.html", "Model Deployment Q&amp;A Guide ", " Model Deployment Q&amp;A Guide Last updated: July 18, 2025 "],["welcome-to-the-cdi-model-deployment-guide.html", "üëã Welcome to the CDI Model Deployment Guide", " üëã Welcome to the CDI Model Deployment Guide Welcome to the Model Deployment domain of the Complex Data Insights (CDI) Q&amp;A series ‚Äî where your machine learning models take their final and most impactful step: into the real world. In this guide, you‚Äôll follow a complete, hands-on journey: üß† Train and evaluate models ‚öôÔ∏è Serve models with FastAPI üì° Test endpoints via Swagger UI üì¶ Package with Docker üñ•Ô∏è Integrate with Streamlit or Gradio üåç Prepare for cloud deployment Whether you‚Äôre a developer, researcher, or educator, this Q&amp;A-driven guide will help you: Save and reuse machine learning models effectively Build, test, and scale model APIs with confidence Interpret predictions with meaningful context Generalize your deployment flow for any project or platform We‚Äôll start from the fundamentals and go all the way to production ‚Äî one Q&amp;A at a time. Let‚Äôs bring your models to life. üîßüìäüß† "],["how-do-you-read-the-dataset-from-the-data-folder-before-deployment.html", "Q&A 1 How do you read the dataset from the data/ folder before deployment? 1.1 Explanation 1.2 Python Code 1.3 R Code", " Q&A 1 How do you read the dataset from the data/ folder before deployment? 1.1 Explanation Before deploying any machine learning model, it‚Äôs essential to understand the data it was trained on. This step helps ensure consistent preprocessing, reproducibility, and seamless integration across tools. In the CDI deployment pipeline, we assume that cleaned and prepared data (like Titanic or Iris datasets) is stored in a data/ folder at the project root. This structure allows for organized workflows and compatibility with scripts and APIs. We‚Äôll demonstrate how to read a typical dataset using both Python and R, preparing it for evaluation or serving. 1.2 Python Code import pandas as pd # Load the Titanic dataset df = pd.read_csv(&quot;data/titanic.csv&quot;) # Preview the first few rows print(df.head()) PassengerId Survived Pclass \\ 0 1 0 3 1 2 1 1 2 3 1 3 3 4 1 1 4 5 0 3 Name Sex Age SibSp \\ 0 Braund, Mr. Owen Harris male 22.0 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 2 Heikkinen, Miss. Laina female 26.0 0 3 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 4 Allen, Mr. William Henry male 35.0 0 Parch Ticket Fare Cabin Embarked 0 0 A/5 21171 7.2500 NaN S 1 0 PC 17599 71.2833 C85 C 2 0 STON/O2. 3101282 7.9250 NaN S 3 0 113803 53.1000 C123 S 4 0 373450 8.0500 NaN S 1.3 R Code library(readr) # Load the Titanic dataset df &lt;- read_csv(&quot;data/titanic.csv&quot;) # Preview the first few rows head(df) # A tibble: 6 √ó 12 PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 0 3 Braund‚Ä¶ male 22 1 0 A/5 2‚Ä¶ 7.25 &lt;NA&gt; 2 2 1 1 Cuming‚Ä¶ fema‚Ä¶ 38 1 0 PC 17‚Ä¶ 71.3 C85 3 3 1 3 Heikki‚Ä¶ fema‚Ä¶ 26 0 0 STON/‚Ä¶ 7.92 &lt;NA&gt; 4 4 1 1 Futrel‚Ä¶ fema‚Ä¶ 35 1 0 113803 53.1 C123 5 5 0 3 Allen,‚Ä¶ male 35 0 0 373450 8.05 &lt;NA&gt; 6 6 0 3 Moran,‚Ä¶ male NA 0 0 330877 8.46 &lt;NA&gt; # ‚Ñπ 1 more variable: Embarked &lt;chr&gt; ‚úÖ Takeaway: Store your datasets in a consistent data/ directory and load them early to ensure your models, APIs, and frontends share the same input structure. "],["how-do-you-train-and-save-multiple-models-for-deployment.html", "Q&A 2 How do you train and save multiple models for deployment? 2.1 Explanation 2.2 Python Code 2.3 R Code", " Q&A 2 How do you train and save multiple models for deployment? 2.1 Explanation Once your dataset is loaded and preprocessed, the next step in the deployment pipeline is to train machine learning models and save them for reuse. Saving models allows you to: Avoid retraining every time the API is restarted Load models instantly in production Maintain version control and reproducibility In this example, we‚Äôll use the Titanic dataset and train multiple classification models. We‚Äôll then save each model as a .joblib file into a models/ folder for future deployment. 2.2 Python Code # scripts/train_n_save_models.py import os import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB import joblib # Load and preprocess dataset df = pd.read_csv(&quot;data/titanic.csv&quot;) df.dropna(subset=[&quot;Age&quot;, &quot;Fare&quot;, &quot;Embarked&quot;, &quot;Sex&quot;, &quot;Survived&quot;], inplace=True) df[&quot;Sex&quot;] = df[&quot;Sex&quot;].astype(&quot;category&quot;).cat.codes df[&quot;Embarked&quot;] = df[&quot;Embarked&quot;].astype(&quot;category&quot;).cat.codes X = df[[&quot;Pclass&quot;, &quot;Sex&quot;, &quot;Age&quot;, &quot;Fare&quot;, &quot;Embarked&quot;]] y = df[&quot;Survived&quot;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define models to train models = { &quot;logistic_regression&quot;: LogisticRegression(max_iter=200), &quot;random_forest&quot;: RandomForestClassifier(), &quot;gradient_boosting&quot;: GradientBoostingClassifier(), &quot;svc&quot;: SVC(probability=True), &quot;decision_tree&quot;: DecisionTreeClassifier(), &quot;knn&quot;: KNeighborsClassifier(), &quot;naive_bayes&quot;: GaussianNB() } # Ensure models directory exists os.makedirs(&quot;models&quot;, exist_ok=True) # Train and save each model for name, model in models.items(): model.fit(X_train, y_train) joblib.dump(model, f&quot;models/{name}.joblib&quot;) print(f&quot;‚úÖ Saved: models/{name}.joblib&quot;) ‚úÖ Saved: models/logistic_regression.joblib ‚úÖ Saved: models/random_forest.joblib ‚úÖ Saved: models/gradient_boosting.joblib ‚úÖ Saved: models/svc.joblib ‚úÖ Saved: models/decision_tree.joblib ‚úÖ Saved: models/knn.joblib ‚úÖ Saved: models/naive_bayes.joblib 2.3 R Code # R version not included in this example as the deployment focus uses joblib (.joblib) in Python. # Alternative: Save R models using saveRDS() if needed for Shiny APIs. ‚úÖ Takeaway: Save each trained model in a dedicated models/ folder using a consistent naming scheme. This enables fast, reliable deployment via your API. "],["how-do-you-evaluate-models-before-deployment.html", "Q&A 3 How do you evaluate models before deployment? 3.1 Explanation 3.2 Python Code 3.3 R Code", " Q&A 3 How do you evaluate models before deployment? 3.1 Explanation Before deploying machine learning models, it‚Äôs important to evaluate their performance on unseen test data. This helps you: Compare models based on accuracy, precision, recall, and F1 score Select the best model(s) for deployment Detect overfitting or underfitting Create a summary table for documentation or reporting In this Q&amp;A, we load previously saved models from the models/ folder, evaluate them on test data, and store the results in a single CSV file: evaluation_summary.csv. 3.2 Python Code # scripts/evaluate_models.py import os import joblib import pandas as pd from sklearn.metrics import accuracy_score, classification_report from sklearn.model_selection import train_test_split # Paths MODEL_DIR = &quot;models&quot; DATA_PATH = &quot;data/titanic.csv&quot; OUTPUT_FILE = &quot;data/evaluation_summary.csv&quot; # Load and preprocess Titanic data df = pd.read_csv(DATA_PATH) df = df.dropna(subset=[&quot;Age&quot;, &quot;Fare&quot;, &quot;Embarked&quot;, &quot;Sex&quot;, &quot;Survived&quot;]) df[&quot;Sex&quot;] = df[&quot;Sex&quot;].astype(&quot;category&quot;).cat.codes df[&quot;Embarked&quot;] = df[&quot;Embarked&quot;].astype(&quot;category&quot;).cat.codes df[&quot;Survived&quot;] = df[&quot;Survived&quot;].astype(int) features = [&quot;Pclass&quot;, &quot;Sex&quot;, &quot;Age&quot;, &quot;Fare&quot;, &quot;Embarked&quot;] X = df[features] y = df[&quot;Survived&quot;] # Train/test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Store results results = [] # Evaluate all saved models for filename in os.listdir(MODEL_DIR): if filename.endswith(&quot;.joblib&quot;): model_path = os.path.join(MODEL_DIR, filename) model = joblib.load(model_path) model_name = filename.replace(&quot;.joblib&quot;, &quot;&quot;) y_pred = model.predict(X_test) acc = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred, output_dict=True) # Use macro avg for simplicity precision = report[&quot;macro avg&quot;][&quot;precision&quot;] recall = report[&quot;macro avg&quot;][&quot;recall&quot;] f1 = report[&quot;macro avg&quot;][&quot;f1-score&quot;] results.append({ &quot;Model&quot;: model_name, &quot;Accuracy&quot;: round(acc, 4), &quot;Precision&quot;: round(precision, 4), &quot;Recall&quot;: round(recall, 4), &quot;F1 Score&quot;: round(f1, 4) }) # Save results to CSV results_df = pd.DataFrame(results) results_df.to_csv(OUTPUT_FILE, index=False) print(f&quot;\\n‚úÖ Evaluation summary saved to: {OUTPUT_FILE} see results below:\\n&quot;) print(results_df) ‚úÖ Evaluation summary saved to: data/evaluation_summary.csv see results below: Model Accuracy Precision Recall F1 Score 0 knn 0.6853 0.6841 0.6867 0.6838 1 svc 0.6364 0.6378 0.6109 0.6038 2 logistic_regression 0.7902 0.8057 0.7737 0.7784 3 gradient_boosting 0.7762 0.7858 0.7612 0.7652 4 random_forest 0.7832 0.7837 0.7742 0.7769 5 naive_bayes 0.7692 0.7734 0.7566 0.7600 6 decision_tree 0.6783 0.6746 0.6653 0.6664 3.3 R Code # For a Python-based deployment workflow, use Python for evaluation. # For R-based workflows, use caret::confusionMatrix() or metrics from modelr or yardstick. ‚úÖ Takeaway: Always evaluate your models and store the results before deployment. This ensures you deploy with confidence and clarity. "],["how-do-you-serve-saved-models-as-prediction-endpoints-using-fastapi.html", "Q&A 4 How do you serve saved models as prediction endpoints using FastAPI? 4.1 Explanation 4.2 Python Code (Define FastAPI App) 4.3 R Code", " Q&A 4 How do you serve saved models as prediction endpoints using FastAPI? 4.1 Explanation Once you‚Äôve saved your trained models, the next step is to create an API that loads those models and makes them available for real-time prediction. FastAPI is a lightweight, high-performance framework that‚Äôs ideal for this. In this Q&amp;A, we define a FastAPI app that: Loads all .joblib models from the models/ folder Defines a prediction route /predict/{model_name} Accepts JSON input using a pydantic schema Returns a prediction as a JSON response 4.2 Python Code (Define FastAPI App) # scripts/model_api.py import os import joblib import pandas as pd from fastapi import FastAPI, HTTPException from pydantic import BaseModel # Load models dynamically MODEL_DIR = &quot;models&quot; models = {} for fname in os.listdir(MODEL_DIR): if fname.endswith(&quot;.joblib&quot;): model_name = fname.replace(&quot;.joblib&quot;, &quot;&quot;) model_path = os.path.join(MODEL_DIR, fname) models[model_name] = joblib.load(model_path) # Create FastAPI app app = FastAPI() # Define input schema class InputData(BaseModel): Pclass: int Sex: int Age: float Fare: float Embarked: int # Define output schema class PredictionOutput(BaseModel): model: str prediction: int # Route to list available models @app.get(&quot;/models&quot;) def list_models(): return {&quot;available_models&quot;: list(models.keys())} # Route to predict using any loaded model @app.post(&quot;/predict/{model_name}&quot;, response_model=PredictionOutput) def predict(model_name: str, input_data: InputData): if model_name not in models: raise HTTPException(status_code=404, detail=&quot;Model not found.&quot;) input_df = pd.DataFrame([input_data.dict()]) model = models[model_name] try: prediction = model.predict(input_df)[0] return PredictionOutput(model=model_name, prediction=int(prediction)) except Exception as e: raise HTTPException(status_code=500, detail=str(e)) 4.3 R Code # This deployment workflow is implemented in Python using FastAPI. # For R, consider plumber for serving models as REST APIs. ‚úÖ Takeaway: FastAPI allows you to create scalable prediction endpoints by loading saved models and exposing them through clean, documented routes. "],["how-do-you-run-and-test-your-fastapi-app-using-uvicorn-and-swagger-ui.html", "Q&A 5 How do you run and test your FastAPI app using Uvicorn and Swagger UI? 5.1 Explanation 5.2 Run Command (Terminal) 5.3 Output (Sample) 5.4 Test in Your Browser 5.5 Example JSON Input 5.6 Troubleshooting", " Q&A 5 How do you run and test your FastAPI app using Uvicorn and Swagger UI? 5.1 Explanation After creating your FastAPI app, you need to run it locally to test prediction endpoints. The standard way to run FastAPI is through Uvicorn, an ASGI server that supports fast, async APIs. When you run your app, FastAPI automatically provides an interactive, browser-based Swagger UI at /docs, where you can test endpoints and submit real input. 5.2 Run Command (Terminal) Assuming your FastAPI file is located at scripts/model_api.py and your app instance is named app, run: uvicorn scripts.model_api:app --reload script.model_api is the Python path: folder + filename (without .py) :app refers to the FastAPI() instance inside the file ‚Äìreload enables live reloading during development 5.3 Output (Sample) INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process‚Ä¶ 5.4 Test in Your Browser Go: http://127.0.0.1:8000/docs You‚Äôll see: /models ‚Üí GET endpoint to list all available models /predict/{model_name} ‚Üí POST endpoint with a form to test predictions Click ‚ÄúTry it out‚Äù on any route, fill in the form, and hit Execute to see the response. 5.5 Example JSON Input { ‚ÄúPclass‚Äù: 1, ‚ÄúSex‚Äù: 1, ‚ÄúAge‚Äù: 32.0, ‚ÄúFare‚Äù: 100.0, ‚ÄúEmbarked‚Äù: 2 } 5.6 Troubleshooting If you get Address already in use: lsof -i :8000 # Find the PID kill -9 &lt;PID&gt; # Kill the process using the port Or use an alternate port:¬†like so: uvicorn script.model_api:app --reload --port 8001 ‚úÖ Takeaway: FastAPI‚Äôs auto-generated Swagger UI lets you run and test prediction APIs right from your browser. Uvicorn runs the app, and /docs gives you an instant frontend for debugging. "],["how-do-you-visualize-model-evaluation-results-from-csv.html", "Q&A 6 How do you visualize model evaluation results from CSV? 6.1 Explanation 6.2 Python Code 6.3 R Code", " Q&A 6 How do you visualize model evaluation results from CSV? 6.1 Explanation Once you‚Äôve evaluated and stored model metrics (accuracy, precision, recall, F1) in a CSV file like evaluation_summary.csv, the next step is to visualize them for quick comparison. Visualization helps: - Identify the best-performing model - Spot trade-offs (e.g., higher precision vs lower recall) - Communicate results to others We‚Äôll use Python‚Äôs pandas, matplotlib, and seaborn to create performance bar plots. 6.2 Python Code import warnings warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning) import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load evaluation summary df = pd.read_csv(&quot;evaluation_summary.csv&quot;) # Set style sns.set(style=&quot;whitegrid&quot;) plt.figure(figsize=(10, 6)) # Plot Accuracy sns.barplot(x=&quot;Accuracy&quot;, y=&quot;Model&quot;, data=df, palette=&quot;viridis&quot;) plt.title(&quot;Model Accuracy Comparison&quot;) plt.tight_layout() # plt.savefig(&quot;accuracy_plot.png&quot;) plt.show() # Plot F1 Score plt.figure(figsize=(10, 6)) sns.barplot(x=&quot;F1 Score&quot;, y=&quot;Model&quot;, data=df, palette=&quot;magma&quot;) plt.title(&quot;Model F1 Score Comparison&quot;) plt.tight_layout() # plt.savefig(&quot;f1_score_plot.png&quot;) plt.show() # Compare Precision vs Recall df_melted = df.melt(id_vars=&quot;Model&quot;, value_vars=[&quot;Precision&quot;, &quot;Recall&quot;], var_name=&quot;Metric&quot;, value_name=&quot;Score&quot;) plt.figure(figsize=(10, 6)) sns.barplot(x=&quot;Score&quot;, y=&quot;Model&quot;, hue=&quot;Metric&quot;, data=df_melted, palette=&quot;Set2&quot;) plt.title(&quot;Precision vs Recall per Model&quot;) plt.tight_layout() # plt.savefig(&quot;precision_recall_plot.png&quot;) plt.show() 6.3 R Code # For R visualization, use ggplot2: df &lt;- read.csv(&quot;evaluation_summary.csv&quot;) library(ggplot2) ggplot(df, aes(x = Accuracy, y = reorder(Model, Accuracy))) + geom_col(fill = &quot;steelblue&quot;) + theme_minimal() + labs(title = &quot;Model Accuracy Comparison&quot;) ‚úÖ Takeaway: Visualizing model metrics makes it easier to select, explain, and justify your deployment choice. Graphs tell the story your numbers started. "],["docker-image-template.html", "Q&A 7 How do you get started with Docker before containerizing your model? 7.1 Explanation 7.2 Step 1: Install Docker Desktop 7.3 Step 2: Start Docker Desktop 7.4 Step 3: Confirm Docker is Active 7.5 Step 4: Write Your First Dockerfile 7.6 Step 5: Build and Run Your First Container 7.7 Step 6: Log in to Docker Hub 7.8 Step 7: Tag and Push Your Image 7.9 Step 8: Scan Your Image with Docker Scout (Optional) 7.10 Example Log Insights", " Q&A 7 How do you get started with Docker before containerizing your model? 7.1 Explanation Docker is a tool that lets you package your application ‚Äî including code, dependencies, and environment ‚Äî into a portable container. Containers run the same way across laptops, servers, and the cloud, making them perfect for deploying machine learning APIs. Before containerizing your FastAPI model, you should first: Understand what Docker is Install and start Docker Desktop Build and run a basic Docker image 7.2 Step 1: Install Docker Desktop üëâ Download Docker for Mac üëâ Download Docker for Windows Install it like any other application. 7.3 Step 2: Start Docker Desktop After installation: Mac: Open Docker from Launchpad or the Applications folder Windows: Open Docker Desktop from the Start menu You should see the üê≥ Docker whale icon appear in your menu bar or system tray. ‚úÖ Wait for the message ‚û°Ô∏è Docker is running 7.4 Step 3: Confirm Docker is Active In your terminal, run: docker info You should see details like Docker version, containers, and storage. If you see an error like: Cannot connect to the Docker daemon‚Ä¶ That means Docker is not running ‚Äî go back and start Docker Desktop. 7.5 Step 4: Write Your First Dockerfile Create a folder named hello-docker, and inside it create a Dockerfile: FROM python:3.12-slim WORKDIR /app COPY . . CMD [&quot;python3&quot;, &quot;-c&quot;, &quot;print(&#39;üéâ Hello from Docker!&#39;)&quot;] 7.6 Step 5: Build and Run Your First Container docker build -t hello-docker . docker run hello-docker You should see: üéâ Hello from Docker! 7.7 Step 6: Log in to Docker Hub Before pushing images or scanning them, log in: docker login You can use: Your Docker ID or email Your password (not recommended) Or a Personal Access Token (PAT) (recommended) üîê Create a PAT here Using a PAT is required if: You have 2FA enabled You‚Äôre part of an organization using SSO You want better security 7.8 Step 7: Tag and Push Your Image After login: docker tag hello-docker yourusername/hello-docker docker push yourusername/hello-docker This makes your image accessible from the cloud or collaborators. 7.9 Step 8: Scan Your Image with Docker Scout (Optional) After building an image, Docker may suggest: View a summary of image vulnerabilities and recommendations ‚Üí docker scout quickview docker scout quickview Docker Scout checks for: Vulnerabilities in your base image Outdated dependencies Upgrade suggestions Make sure you‚Äôre logged in before using Scout 7.10 Example Log Insights Cannot connect to the Docker daemon‚Ä¶ ‚Üí Start Docker Desktop first and retry. Using a limited-scope PAT grants better security.. ‚Üí Prefer PATs for login over full passwords. ‚úÖ Takeaway: Understanding Docker basics ‚Äî installation, login, and image scanning ‚Äî prepares you for robust, secure ML model deployment and reproducibility.. "],["how-do-you-containerize-your-model-api-using-docker-for-reproducible-deployment.html", "Q&A 8 How do you containerize your model API using Docker for reproducible deployment? 8.1 Explanation 8.2 Project Structure 8.3 Dockerfile 8.4 Libraries in requirements.txt (Minimum Needed) 8.5 Build the Docker Image 8.6 Run the Docker Container 8.7 Optional: .dockerignore", " Q&A 8 How do you containerize your model API using Docker for reproducible deployment? 8.1 Explanation Docker allows you to package your model API and dependencies into a container that runs the same way on any machine. This makes your deployment: Portable across teams and clouds Reproducible and isolated from system conflicts Easy to scale or integrate with CI/CD pipelines We‚Äôll build a Docker container for your FastAPI model API that loads saved .joblib models and runs with Uvicorn. 8.2 Project Structure cdi-model-deployment/ ‚îú‚îÄ‚îÄ script/ ‚îÇ ‚îî‚îÄ‚îÄ model_api.py ‚îú‚îÄ‚îÄ models/ ‚îÇ ‚îî‚îÄ‚îÄ [saved models here] ‚îú‚îÄ‚îÄ requirements.txt ‚îî‚îÄ‚îÄ Dockerfile 8.3 Dockerfile # Base image FROM python:3.12-slim # Set working directory WORKDIR /app # Copy code and model directory COPY script/model_api.py ./script/model_api.py COPY models/ ./models/ COPY requirements.txt . # Install dependencies RUN pip install --no-cache-dir -r requirements.txt # Expose port for Uvicorn EXPOSE 8000 # Run the API with Uvicorn CMD [&quot;uvicorn&quot;, &quot;script.model_api:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8000&quot;] 8.4 Libraries in requirements.txt (Minimum Needed) fastapi==0.115.4 uvicorn==0.35.0 joblib==1.4.2 scikit-learn==1.6.0 pandas==2.2.3 gradio==5.9.1 streamlit==1.39.0 8.5 Build the Docker Image docker build -t model-api . 8.6 Run the Docker Container docker run -p 8000:8000 model-api Then test it at: http://127.0.0.1:8000/docs 8.7 Optional: .dockerignore To avoid copying your local venv or large files: __pycache__/ venv/ *.csv *.ipynb ‚úÖ Takeaway: Docker lets you containerize your model API so it runs identically on any machine ‚Äî a must-have for production deployment. "],["how-do-you-tag-and-push-your-model-image-to-docker-hub.html", "Q&A 9 How do you tag and push your model image to Docker Hub? 9.1 üìò Explanation 9.2 Log into Docker Hub 9.3 Tag the image using your Docker Hub username 9.4 Push the image to Docker Hub Confirm image available publicly or privately", " Q&A 9 How do you tag and push your model image to Docker Hub? 9.1 üìò Explanation Once you‚Äôve built your Docker image locally (e.g., model-api:latest), you can share it with others or deploy it remotely by tagging it with your Docker Hub name and pushing it to your Docker Hub repository. This makes your image accessible from anywhere using docker pull. 9.2 Log into Docker Hub docker login You‚Äôll be prompted to enter your Docker Hub username and password or Personal Access Token (PAT). 9.3 Tag the image using your Docker Hub username docker tag model-api your_dockerhub_username/model-api üîÅ Replace your_dockerhub_username with your Docker Hub username. Example: docker tag model-api tmbuza/model-api 9.4 Push the image to Docker Hub docker push your_dockerhub_username/model-api Example: docker push tmbuza/model-api Docker will upload the image layers and confirm when complete. Confirm image available publicly or privately Visit your Docker Hub account and check under Repositories. ‚úÖ You (or others) can now pull the image with: docker pull your_dockerhub_username/model-api Example: docker pull tmbuza/model-api ‚úÖ Takeaway: Tagging and pushing your image makes it accessible for deployment, collaboration, or reuse‚Äîespecially valuable in reproducible ML workflows. "],["how-do-you-pull-and-run-your-model-api-image-from-docker-hub.html", "Q&A 10 How do you pull and run your model API image from Docker Hub? 10.1 Explanation 10.2 Bash Code 10.3 Run the Docker container 10.4 Test the running API", " Q&A 10 How do you pull and run your model API image from Docker Hub? 10.1 Explanation After pushing your trained model image to Docker Hub, you can reuse it on any machine or server with Docker installed. This step is key in model deployment because it enables reproducibility, portability, and team collaboration‚Äîyou don‚Äôt have to re-train or re-build the image every time. 10.2 Bash Code 10.2.1 Pull the image from Docker Hub xample using tnbuza username docker pull tmbuza/model-api 10.3 Run the Docker container docker run -d -p 8000:8000 tmbuza/model-api Explanation: -d = run in detached mode -p 8000:8000 = map internal container port 8000 to local machine port 8000 10.4 Test the running API Visit the following URL in your browser: http://127.0.0.1:8000/docs You should see the interactive FastAPI Swagger UI to test predictions directly. ‚úÖ Takeaway: Pulling and running your model API from Docker Hub turns it into a reusable and portable microservice ‚Äî ideal for deployment on servers, sharing with teams, or testing in production-like environments. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
