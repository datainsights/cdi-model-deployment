[["index.html", "Model Deployment Q&amp;A Guide ", " Model Deployment Q&amp;A Guide Last updated: July 18, 2025 "],["welcome-to-the-cdi-model-deployment-guide.html", "üëã Welcome to the CDI Model Deployment Guide", " üëã Welcome to the CDI Model Deployment Guide Welcome to the Model Deployment domain of the Complex Data Insights (CDI) Q&amp;A series ‚Äî where your machine learning models take their final and most impactful step: into the real world. In this guide, you‚Äôll follow a complete, hands-on journey: üß† Train and evaluate models ‚öôÔ∏è Serve models with FastAPI üì° Test endpoints via Swagger UI üì¶ Package with Docker üñ•Ô∏è Integrate with Streamlit or Gradio üåç Prepare for cloud deployment Whether you‚Äôre a developer, researcher, or educator, this Q&amp;A-driven guide will help you: Save and reuse machine learning models effectively Build, test, and scale model APIs with confidence Interpret predictions with meaningful context Generalize your deployment flow for any project or platform We‚Äôll start from the fundamentals and go all the way to production ‚Äî one Q&amp;A at a time. Let‚Äôs bring your models to life. üîßüìäüß† "],["how-do-you-read-the-dataset-from-the-data-folder-before-deployment.html", "Q&A 1 How do you read the dataset from the data/ folder before deployment? 1.1 Explanation 1.2 Python Code 1.3 R Code", " Q&A 1 How do you read the dataset from the data/ folder before deployment? 1.1 Explanation Before deploying any machine learning model, it‚Äôs essential to understand the data it was trained on. This step helps ensure consistent preprocessing, reproducibility, and seamless integration across tools. In the CDI deployment pipeline, we assume that cleaned and prepared data (like Titanic or Iris datasets) is stored in a data/ folder at the project root. This structure allows for organized workflows and compatibility with scripts and APIs. We‚Äôll demonstrate how to read a typical dataset using both Python and R, preparing it for evaluation or serving. 1.2 Python Code import pandas as pd # Load the Titanic dataset df = pd.read_csv(&quot;data/titanic.csv&quot;) # Preview the first few rows print(df.head()) PassengerId Survived Pclass \\ 0 1 0 3 1 2 1 1 2 3 1 3 3 4 1 1 4 5 0 3 Name Sex Age SibSp \\ 0 Braund, Mr. Owen Harris male 22.0 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 2 Heikkinen, Miss. Laina female 26.0 0 3 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 4 Allen, Mr. William Henry male 35.0 0 Parch Ticket Fare Cabin Embarked 0 0 A/5 21171 7.2500 NaN S 1 0 PC 17599 71.2833 C85 C 2 0 STON/O2. 3101282 7.9250 NaN S 3 0 113803 53.1000 C123 S 4 0 373450 8.0500 NaN S 1.3 R Code library(readr) # Load the Titanic dataset df &lt;- read_csv(&quot;data/titanic.csv&quot;) # Preview the first few rows head(df) # A tibble: 6 √ó 12 PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 1 0 3 Braund‚Ä¶ male 22 1 0 A/5 2‚Ä¶ 7.25 &lt;NA&gt; 2 2 1 1 Cuming‚Ä¶ fema‚Ä¶ 38 1 0 PC 17‚Ä¶ 71.3 C85 3 3 1 3 Heikki‚Ä¶ fema‚Ä¶ 26 0 0 STON/‚Ä¶ 7.92 &lt;NA&gt; 4 4 1 1 Futrel‚Ä¶ fema‚Ä¶ 35 1 0 113803 53.1 C123 5 5 0 3 Allen,‚Ä¶ male 35 0 0 373450 8.05 &lt;NA&gt; 6 6 0 3 Moran,‚Ä¶ male NA 0 0 330877 8.46 &lt;NA&gt; # ‚Ñπ 1 more variable: Embarked &lt;chr&gt; ‚úÖ Takeaway: Store your datasets in a consistent data/ directory and load them early to ensure your models, APIs, and frontends share the same input structure. "],["how-do-you-train-and-save-multiple-models-for-deployment.html", "Q&A 2 How do you train and save multiple models for deployment? 2.1 Explanation 2.2 Python Code 2.3 R Code", " Q&A 2 How do you train and save multiple models for deployment? 2.1 Explanation Once your dataset is loaded and preprocessed, the next step in the deployment pipeline is to train machine learning models and save them for reuse. Saving models allows you to: Avoid retraining every time the API is restarted Load models instantly in production Maintain version control and reproducibility In this example, we‚Äôll use the Titanic dataset and train multiple classification models. We‚Äôll then save each model as a .joblib file into a models/ folder for future deployment. 2.2 Python Code # scripts/train_n_save_models.py import os import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB import joblib # Load and preprocess dataset df = pd.read_csv(&quot;data/titanic.csv&quot;) df.dropna(subset=[&quot;Age&quot;, &quot;Fare&quot;, &quot;Embarked&quot;, &quot;Sex&quot;, &quot;Survived&quot;], inplace=True) df[&quot;Sex&quot;] = df[&quot;Sex&quot;].astype(&quot;category&quot;).cat.codes df[&quot;Embarked&quot;] = df[&quot;Embarked&quot;].astype(&quot;category&quot;).cat.codes X = df[[&quot;Pclass&quot;, &quot;Sex&quot;, &quot;Age&quot;, &quot;Fare&quot;, &quot;Embarked&quot;]] y = df[&quot;Survived&quot;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define models to train models = { &quot;logistic_regression&quot;: LogisticRegression(max_iter=200), &quot;random_forest&quot;: RandomForestClassifier(), &quot;gradient_boosting&quot;: GradientBoostingClassifier(), &quot;svc&quot;: SVC(probability=True), &quot;decision_tree&quot;: DecisionTreeClassifier(), &quot;knn&quot;: KNeighborsClassifier(), &quot;naive_bayes&quot;: GaussianNB() } # Ensure models directory exists os.makedirs(&quot;models&quot;, exist_ok=True) # Train and save each model for name, model in models.items(): model.fit(X_train, y_train) joblib.dump(model, f&quot;models/{name}.joblib&quot;) print(f&quot;‚úÖ Saved: models/{name}.joblib&quot;) ‚úÖ Saved: models/logistic_regression.joblib ‚úÖ Saved: models/random_forest.joblib ‚úÖ Saved: models/gradient_boosting.joblib ‚úÖ Saved: models/svc.joblib ‚úÖ Saved: models/decision_tree.joblib ‚úÖ Saved: models/knn.joblib ‚úÖ Saved: models/naive_bayes.joblib 2.3 R Code # R version not included in this example as the deployment focus uses joblib (.joblib) in Python. # Alternative: Save R models using saveRDS() if needed for Shiny APIs. ‚úÖ Takeaway: Save each trained model in a dedicated models/ folder using a consistent naming scheme. This enables fast, reliable deployment via your API. "],["how-do-you-evaluate-models-before-deployment.html", "Q&A 3 How do you evaluate models before deployment? 3.1 Explanation 3.2 Python Code 3.3 R Code", " Q&A 3 How do you evaluate models before deployment? 3.1 Explanation Before deploying machine learning models, it‚Äôs important to evaluate their performance on unseen test data. This helps you: Compare models based on accuracy, precision, recall, and F1 score Select the best model(s) for deployment Detect overfitting or underfitting Create a summary table for documentation or reporting In this Q&amp;A, we load previously saved models from the models/ folder, evaluate them on test data, and store the results in a single CSV file: evaluation_summary.csv. 3.2 Python Code # scripts/evaluate_models.py import os import joblib import pandas as pd from sklearn.metrics import accuracy_score, classification_report from sklearn.model_selection import train_test_split # Paths MODEL_DIR = &quot;models&quot; DATA_PATH = &quot;data/titanic.csv&quot; OUTPUT_FILE = &quot;data/evaluation_summary.csv&quot; # Load and preprocess Titanic data df = pd.read_csv(DATA_PATH) df = df.dropna(subset=[&quot;Age&quot;, &quot;Fare&quot;, &quot;Embarked&quot;, &quot;Sex&quot;, &quot;Survived&quot;]) df[&quot;Sex&quot;] = df[&quot;Sex&quot;].astype(&quot;category&quot;).cat.codes df[&quot;Embarked&quot;] = df[&quot;Embarked&quot;].astype(&quot;category&quot;).cat.codes df[&quot;Survived&quot;] = df[&quot;Survived&quot;].astype(int) features = [&quot;Pclass&quot;, &quot;Sex&quot;, &quot;Age&quot;, &quot;Fare&quot;, &quot;Embarked&quot;] X = df[features] y = df[&quot;Survived&quot;] # Train/test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Store results results = [] # Evaluate all saved models for filename in os.listdir(MODEL_DIR): if filename.endswith(&quot;.joblib&quot;): model_path = os.path.join(MODEL_DIR, filename) model = joblib.load(model_path) model_name = filename.replace(&quot;.joblib&quot;, &quot;&quot;) y_pred = model.predict(X_test) acc = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred, output_dict=True) # Use macro avg for simplicity precision = report[&quot;macro avg&quot;][&quot;precision&quot;] recall = report[&quot;macro avg&quot;][&quot;recall&quot;] f1 = report[&quot;macro avg&quot;][&quot;f1-score&quot;] results.append({ &quot;Model&quot;: model_name, &quot;Accuracy&quot;: round(acc, 4), &quot;Precision&quot;: round(precision, 4), &quot;Recall&quot;: round(recall, 4), &quot;F1 Score&quot;: round(f1, 4) }) # Save results to CSV results_df = pd.DataFrame(results) results_df.to_csv(OUTPUT_FILE, index=False) print(f&quot;\\n‚úÖ Evaluation summary saved to: {OUTPUT_FILE} see results below:\\n&quot;) print(results_df) ‚úÖ Evaluation summary saved to: data/evaluation_summary.csv see results below: Model Accuracy Precision Recall F1 Score 0 knn 0.6853 0.6841 0.6867 0.6838 1 svc 0.6364 0.6378 0.6109 0.6038 2 logistic_regression 0.7902 0.8057 0.7737 0.7784 3 gradient_boosting 0.7762 0.7858 0.7612 0.7652 4 random_forest 0.7762 0.7773 0.7663 0.7692 5 naive_bayes 0.7692 0.7734 0.7566 0.7600 6 decision_tree 0.6993 0.6956 0.6891 0.6906 3.3 R Code # For a Python-based deployment workflow, use Python for evaluation. # For R-based workflows, use caret::confusionMatrix() or metrics from modelr or yardstick. ‚úÖ Takeaway: Always evaluate your models and store the results before deployment. This ensures you deploy with confidence and clarity. "],["how-do-you-serve-saved-models-as-prediction-endpoints-using-fastapi.html", "Q&A 4 How do you serve saved models as prediction endpoints using FastAPI? 4.1 Explanation 4.2 Python Code (Define FastAPI App) 4.3 R Code", " Q&A 4 How do you serve saved models as prediction endpoints using FastAPI? 4.1 Explanation Once you‚Äôve saved your trained models, the next step is to create an API that loads those models and makes them available for real-time prediction. FastAPI is a lightweight, high-performance framework that‚Äôs ideal for this. In this Q&amp;A, we define a FastAPI app that: Loads all .joblib models from the models/ folder Defines a prediction route /predict/{model_name} Accepts JSON input using a pydantic schema Returns a prediction as a JSON response 4.2 Python Code (Define FastAPI App) # scripts/model_api.py import os import joblib import pandas as pd from fastapi import FastAPI, HTTPException from pydantic import BaseModel # Load models dynamically MODEL_DIR = &quot;models&quot; models = {} for fname in os.listdir(MODEL_DIR): if fname.endswith(&quot;.joblib&quot;): model_name = fname.replace(&quot;.joblib&quot;, &quot;&quot;) model_path = os.path.join(MODEL_DIR, fname) models[model_name] = joblib.load(model_path) # Create FastAPI app app = FastAPI() # Define input schema class InputData(BaseModel): Pclass: int Sex: int Age: float Fare: float Embarked: int # Define output schema class PredictionOutput(BaseModel): model: str prediction: int # Route to list available models @app.get(&quot;/models&quot;) def list_models(): return {&quot;available_models&quot;: list(models.keys())} # Route to predict using any loaded model @app.post(&quot;/predict/{model_name}&quot;, response_model=PredictionOutput) def predict(model_name: str, input_data: InputData): if model_name not in models: raise HTTPException(status_code=404, detail=&quot;Model not found.&quot;) input_df = pd.DataFrame([input_data.dict()]) model = models[model_name] try: prediction = model.predict(input_df)[0] return PredictionOutput(model=model_name, prediction=int(prediction)) except Exception as e: raise HTTPException(status_code=500, detail=str(e)) 4.3 R Code # This deployment workflow is implemented in Python using FastAPI. # For R, consider plumber for serving models as REST APIs. ‚úÖ Takeaway: FastAPI allows you to create scalable prediction endpoints by loading saved models and exposing them through clean, documented routes. "],["how-do-you-run-and-test-your-fastapi-app-using-uvicorn-and-swagger-ui.html", "Q&A 5 How do you run and test your FastAPI app using Uvicorn and Swagger UI? 5.1 Explanation 5.2 Run Command (Terminal) 5.3 Output (Sample) 5.4 Test in Your Browser 5.5 Example JSON Input 5.6 Troubleshooting", " Q&A 5 How do you run and test your FastAPI app using Uvicorn and Swagger UI? 5.1 Explanation After creating your FastAPI app, you need to run it locally to test prediction endpoints. The standard way to run FastAPI is through Uvicorn, an ASGI server that supports fast, async APIs. When you run your app, FastAPI automatically provides an interactive, browser-based Swagger UI at /docs, where you can test endpoints and submit real input. 5.2 Run Command (Terminal) Assuming your FastAPI file is located at scripts/model_api.py and your app instance is named app, run: uvicorn scripts.model_api:app --reload script.model_api is the Python path: folder + filename (without .py) :app refers to the FastAPI() instance inside the file ‚Äìreload enables live reloading during development 5.3 Output (Sample) INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) INFO: Started reloader process‚Ä¶ 5.4 Test in Your Browser Go: http://127.0.0.1:8000/docs You‚Äôll see: /models ‚Üí GET endpoint to list all available models /predict/{model_name} ‚Üí POST endpoint with a form to test predictions Click ‚ÄúTry it out‚Äù on any route, fill in the form, and hit Execute to see the response. 5.5 Example JSON Input { ‚ÄúPclass‚Äù: 1, ‚ÄúSex‚Äù: 1, ‚ÄúAge‚Äù: 32.0, ‚ÄúFare‚Äù: 100.0, ‚ÄúEmbarked‚Äù: 2 } 5.6 Troubleshooting If you get Address already in use: lsof -i :8000 # Find the PID kill -9 &lt;PID&gt; # Kill the process using the port Or use an alternate port:¬†like so: uvicorn script.model_api:app --reload --port 8001 ‚úÖ Takeaway: FastAPI‚Äôs auto-generated Swagger UI lets you run and test prediction APIs right from your browser. Uvicorn runs the app, and /docs gives you an instant frontend for debugging. "],["how-do-you-visualize-model-evaluation-results-from-csv.html", "Q&A 6 How do you visualize model evaluation results from CSV? 6.1 Explanation 6.2 Python Code 6.3 R Code", " Q&A 6 How do you visualize model evaluation results from CSV? 6.1 Explanation Once you‚Äôve evaluated and stored model metrics (accuracy, precision, recall, F1) in a CSV file like evaluation_summary.csv, the next step is to visualize them for quick comparison. Visualization helps: - Identify the best-performing model - Spot trade-offs (e.g., higher precision vs lower recall) - Communicate results to others We‚Äôll use Python‚Äôs pandas, matplotlib, and seaborn to create performance bar plots. 6.2 Python Code import warnings warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning) import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load evaluation summary df = pd.read_csv(&quot;evaluation_summary.csv&quot;) # Set style sns.set(style=&quot;whitegrid&quot;) plt.figure(figsize=(10, 6)) # Plot Accuracy sns.barplot(x=&quot;Accuracy&quot;, y=&quot;Model&quot;, data=df, palette=&quot;viridis&quot;) plt.title(&quot;Model Accuracy Comparison&quot;) plt.tight_layout() # plt.savefig(&quot;accuracy_plot.png&quot;) plt.show() # Plot F1 Score plt.figure(figsize=(10, 6)) sns.barplot(x=&quot;F1 Score&quot;, y=&quot;Model&quot;, data=df, palette=&quot;magma&quot;) plt.title(&quot;Model F1 Score Comparison&quot;) plt.tight_layout() # plt.savefig(&quot;f1_score_plot.png&quot;) plt.show() # Compare Precision vs Recall df_melted = df.melt(id_vars=&quot;Model&quot;, value_vars=[&quot;Precision&quot;, &quot;Recall&quot;], var_name=&quot;Metric&quot;, value_name=&quot;Score&quot;) plt.figure(figsize=(10, 6)) sns.barplot(x=&quot;Score&quot;, y=&quot;Model&quot;, hue=&quot;Metric&quot;, data=df_melted, palette=&quot;Set2&quot;) plt.title(&quot;Precision vs Recall per Model&quot;) plt.tight_layout() # plt.savefig(&quot;precision_recall_plot.png&quot;) plt.show() 6.3 R Code # For R visualization, use ggplot2: df &lt;- read.csv(&quot;evaluation_summary.csv&quot;) library(ggplot2) ggplot(df, aes(x = Accuracy, y = reorder(Model, Accuracy))) + geom_col(fill = &quot;steelblue&quot;) + theme_minimal() + labs(title = &quot;Model Accuracy Comparison&quot;) ‚úÖ Takeaway: Visualizing model metrics makes it easier to select, explain, and justify your deployment choice. Graphs tell the story your numbers started. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
